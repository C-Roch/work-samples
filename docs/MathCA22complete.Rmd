---
title: "Math of Correspondence Analysis"
author: "Dr. Claudia Roch M.A."
date: "2022-11-08"
output:
  html_document:
    code_download: yes
    df_print: paged
    fig_caption: yes
    self_contained: False
    pandoc_args: 
      - --mathjax=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/latest.min.js?config=TeX-MML-AM_HTMLorMML.js # include mathjax
    toc: yes    # index
    toc_depth: 4
editor_options:
  chunk_output_type: inline
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Math of Correspondence Analysis - Case Study "Pragmatic Extenders used by English Teenagers"

# Sources & Preparation

This notebook has been created for the [Exploratory Data Analysis](https://vvz.ruhr-uni-bochum.de/campus/all/event.asp?objgguid=NEW&from=vvz&gguid=0xE2AC2ABDE5C3474A92007FEE6D3FECBA&mode=own&tguid=0xBEC4EBD3E08E451BB6DEBD69F230152F&lang=en) Advanced Course at Linguistic Data Science Lab, Ruhr University Bochum.

The primary aim of this notebook is to follow along the mathematical foundations of correspondence analysis. We "manually" calculate all coordinates and summary statistics and compare the values to the output of pre-compiled `ca`-packages.

The secondary aim is to explore a case study that deals with a linguistic phenomenon - the local dialect in different social classes of English teenagers from three local cities.

**Sources:**

The notebook is based on the following sources:

- Desagulier (2017), chapter 10, see also [additional materials](http://extras.springer.com/2017/978-3-319-64570-4).

- The part on the mathematical foundation of CA relies on a [script by Tim Bock](<https://www.displayr.com/math-correspondence-analysis/>).



**Required R packages:** `ca`, `data.table`
```{r}
library("ca"); library("data.table")
```



# 0. Case Study - Pragmatic Extenders used by English Teenagers


We use the `extenders` dataset following a case study as presented in ch. 10 of Desagulier (2017). The original dataset was recorded by Cheshire (2007) and contains frequencies for pragmatic extenders (*and that*, *and all that*, *and stuff*, *and things*, *and everything*, *or something*).

> General extenders are clause-final pragmatic particles appended to a word, a phrase, or a clause (Cheshire 2007, p. 156). 

> Their basic pattern is a conjunction (*and* or *but*) followed by a noun phrase. (Desagulier 2017:257)

The following examples (after Desagulier 2017:257) illustrate their usage:

* *Steve works in the engineering office and has taken over some of the er you know purchasing function as well, like enquiries **and stuff**.* (BNC–JP2)
* *And I don’t think we do it so much with the erm (pause) careers service training, but with the careers teachers often we’ve got them doing action plans **and things**.* (BNC–G4X)
* *I knew exactly where to stop it so that I could get off and go down the ladder, sneak a cup of tea **or something**.* (BNC–FXV)

Following Desagulier (2017), we aim to explore the use of extenders in the language of English teenagers from different social classes (working class (WC) or middle class(MC)) from three English towns (Reading, Milton-Keynes, Hull) applying correspondence analysis. The towns are located in three distinct regional locations in England. 
Cheshire (2007:164) argues that

>“there [is] a robust socialclass distinction in the use of certain forms (. . . )” 

The variables for correspondence analysis are hence **pragmatic extender** vs. the conflated variable **Region_social class** of the teenagers.

Before we take a look at the data and explore it by applying correspondence analysis, let us formulate a few research questions: 

## 0.1 Research questions

* Are there geographic/social differences across the data
set? And, if so, what are these differences?

* Which linguistic forms have the same regional and social profiles?

* What are the linguistic profiles of each region/social class with respect to general extenders?

## 0.2 Dataset

```{r}
# read rds file which stores the dataset as a single R object
extendersdata <- readRDS("df_extenders.rds")
extendersdata
```

```{r}
class(extendersdata)
```


The dataset's format is a **contingency table**. 

> A contingency table or crosstab indicates, for two or more categorical variables, how frequently the combinations of characteristic values occur.

The frequencies in the rows are referred to as n~ij~ where i numbers the rows 1 to I (I=6) and j numbers the columns 1 to J (J=6). In the data matrix IxJ e.g. we grasp that n~42~ = 0.

```{r}
N = extendersdata
I <- nrow(N)
J <- ncol(N)
I
J
N[4,2]
```
The values of a variable (e.g. '*and that*') are referred to as points (in case of '*and that*' it is actually a row point.) Row points and column points can be depicted in a joint space in a map.

## 0.3 Performing statistical tests

The background of performing statistical tests is that we want to check whether the rows and columns are not independent (this means basically whether the research question is reasonable at all), although theoretically in exploratory analyses, we would not make assumptions regarding groupings to be found.


### 0.3.1 Chi^2-test

The χ 2 test checks the significance of the overall deviation of the
data in the contingency table from the independence model. It computes the contribution of each cell to χ 2 and sums up all contributions. 

```{r}
chisq <- chisq.test(extendersdata)
chisq
```
[Eventually, consider a tool for converting scientific notation e.g. https://calculator.name/scientific-notation/scientific-notation-to-decimal.php]


Here, χ 2 has a high value and it is associated with very small p-value (0.00000000000000022). This implies that the row and column variables are not independent and that there is a relationship. However, the magnitude of the χ 2 value should not be interpreted as quantifying the effect of the correlation, because it depends on the sample size. 

How do we know if the determined Chi-square value is a large value?

We can use statistics tables with the Chi-squared distribution to compare our values, like e.g. <https://en.wikibooks.org/wiki/Engineering_Tables/Chi-Squared_Distibution>.
We have to check the cell for the corresponding degrees of freedom (see df=25 value in output) and the assumed significance level (e.g. 0.05.). If the calculated chi-square value is smaller than the critical value, the null hypothesis can be retained.


For an IxJ table (6x6=36), the number of degrees of freedom is given by (I-1)x(J-1) or (5x5=25)). Degrees of freedom refers to the number of values that can be varied without changing the relevant statistical parameter. 
For example, if you take the arithmetic mean of three values, then you can freely vary two and get the same result, the last value cannot be freely varied, because it is already determined.

$$1+2+3/3 = 2$$
$$2+4+0/3 = 2$$
but not $$2+4+1$$


As the most important significance levels  \alpha we distinguish: 
0,05  (< = 5% significant), (95%)
0,01 (< = 1% very significant), (99%)
0,001(< = 0,1% highly significant) (99,9%).

The test statistic is usually associated with a P-value which indicates the small probability (0.001 corresponds to 1 in 1000) of yet reconciling the observed frequencies with the independence hypothesis. 
The P-value indicates how incompatible data are with a particular statistical model: the smaller the value, the greater the incompatibility with the null hypothesis. P-values do not measure the probability that a hypothesis under investigation is true (see Wasserstein & Lazar 2016).


In general the test imposes conditions that all observations are independent and that 80+% of the expected frequencies are larger than 5.


```{r}
chisq$exp
```

In our case, the dataset does not meet the second assumption  (only 67% of the sample size is greater 5) but Greenacre (2007) argues that 
it can be applied . Given very small p-value, the significance of the deviation of the table from independence is not deniable. 


Later on, in section 3.1, we will see how to calculate the chi-square value manually, with some more in depth discussion.  


### 0.3.2 Pearson's residuals

Pearson's r is a correlation measure for the association between two random variables (x and y).

If the Pearson residual in a cell is positive/negative, then the observed frequency in that cell is greater/less than the expected frequency in that cell. Second, the more the Pearson residual deviates from 0, the stronger that effect, or, in other words, the closer the coefficient is to 0, the weaker the correlation.

```{r}
chisq$res
```

Upon inspection of the values we observe a strong effect for:

* '*and that*' - '*Reading_MC*' (-5.69), - '*Milton_Keynes_MC*' (-4.27),- '*Milton_Keynes_WC*' (5.09), - '*Hull_WC*' (6.57)
* '*or something*' - '*Reading_MC*' (5.95), - '*Hull_WC*' (-4.64)
* '*and all that*' - '*Reading_WC*' (5.05)
* '*and things*' - '*Milton_Keynes_MC*' (4.91)
* '*and stuff*' - '*Hull_MC*' (5.61)
...

### 0.3.3 Cramer's V: intensity of the relationship 

The intensity of the relationship is measured with Cramér’s V, and it is non negligible for the dataset. The perfect score of 1 is unrealistic here, as the association between the use of extenders and the socio-geographic background is not exclusive (cf. Desagulier 2017:269).
```{r}
Vdif <- sqrt(chisq$stat/(sum(extendersdata) * (min(dim(extendersdata))-1)))
round(Vdif, 3)

```

See also section 3.1 for the formula how to calculate it.

## 0.4 A first look at ca analysis and interpretation

Before we proceed with complex mathematical operations, let us have a first look at what a CA analysis of the dataset looks and what initial observations can be made. 

Without the help of a CA graph, Cheshire (2007, p. 164) interprets the table of frequencies as follows:

>Of the adjunctives, *and that* was preferred by the working-class speakers in all three towns, as was the less frequent *and all that*. The middle-class speakers, on the other hand, preferred *and stuff* and *and things* again in all three towns, though in Hull the middle-class adolescents used *and stuff* far more often than *and things* (the relatively high frequency
of *and stuff* for the working-class group in Hull was due to just three speakers, one of whom was responsible for 10 of the 18 tokens).

Keeping in mind those observations, we will now plot the low-dimensional map resulting from the CA analysis and inspect it.


```{r}
library(ca)
# performing ca with ca package
ca_ext <- ca(extendersdata, graph=F)
par(mfrow=c(1,2)) # set the plotting area into a 1*2 array
# plotting first two dimensions
plot(ca_ext, dim=c(1:2))
# and second and third dimension
plot(ca_ext, dim=c(3:2))

```


When we interpret the map of the first and second dimension, we find 

* a clear divide between the use of extenders by middle-class  and working-class informants on the first axis
    - extenders favored by middle-class teenagers are *or something*, *and things*, and *and stuff*
    - extenders favored by working-class teenagers are *and all that*, and *and that*
    - *and everything* in the middle is well projected along the vertical axis, and seems to be indifferent to socio-geographic groups

* that the vertical axis shows a difference between Hull and Reading representing the geographical dissimilarity with Milton Keynes somewhere in between; Note: Hull is in Yorkshire (northern England), whereas Reading and Milton Keynes are much further south.

When inspecting the second and third dimensional view zooming in a little bit, we observe that

* *and all that* (Freq. R:18, M:6, H:5) and *or something* (Freq. R:92, M:47, H:26) group with Reading
* *and stuff* (Freq. R:42, M:50, H:80) and *and everything* (Freq: R: 37, M: 40, H:61) group with Hull
* *and things* (Freq. R:32, M:35, H:17) and *and that* (Freq. R: 53, M:53, H:76) which are very dissimilar in the first dimension (because of being differently associated with middle and working class) are not separated by the third dimension. They appear close to Milton Keynes but are relatively frequently used in all towns.


In sum, we find that the dimension-reduction technique to represent the (dis-)similarity between variables in a map, yields insights which are in line with the interpretation of raw frequencies. For the moment, we have skipped a look at the numerical results of correspondence analysis which are equally important in interpretation, and we will turn to those when considering the mathematical foundation. 

## 0.5 Validation of calculations with the ``ca`` package

There are different R packages for performing correspondence analysis: throughout this notebook we use Greenacre & Nenadic's (2020) ``ca`` package and apply the ``ca()`` command for simple correspondence analysis.
In the output below, you will see the existing variables of a ca-analysis object. We will load the single predefined variables during our calculations to check whether our manually obtained results are correct.

```{r}
library(ca)
ca_validation <- ca(N)
str(ca_validation)
```

We can also print a condensed summary of the analysis' results with ``summary`` where variables are abbreviated in the output. We will explain the values' significance in short. 

```{r}
# print summary
summary(ca_validation)
```


# 1.0 Observed proportions, profiles and masses

Before we start with the mathematical calculations to obtain observed proportions, we need to transform the raw frequencies in the contingency table, e.g. by adding the margin totals.

## 1.1 Row and Column Totals 

We create the row sums (`rowSums`) and column sums (`colSums`) of N, bind them to the margins of the original table and name the additional row and column.
```{r}

N_total = rbind(N, colSums(N))
N_total = cbind(N_total, rowSums(N_total))
rownames(N_total)[7] <- "columntotal"
colnames(N_total)[7] <- "rowtotal"
N_total
```



## 1.2 Calculating observed proportions P in a correspondence matrix 

In this step, we transform the frequencies of the contingency table into proportions.

Therefore, we first need the total frequency n of all observed instances in table `N` which can be obtained by applying the function `sum()` to the table.
```{r}
n <- sum(N)
n
```

Next, we create the table with the observed proportions `P` through the following formula. To calculate the *correspondence matrix* of relative frequencies p~ij~, frequencies are divided by the total frequency n.


```{r}
P <- N/n

P
```

## 1.3 Profiles and Masses

In this section we will understand the important concepts of profiles and masses.

### 1.3.1 Row and column profiles

> A row, or a column of a contingency table in which elements have been divided by the row sums or column sums respectively. Profiles are visualized as points in space by correspondence analysis.(cf. Hautz & Bleuel 2018:221) 

For the next step, we note that profiles are obtained by creating relative frequencies from the marginal sums instead of dividing them by the total frequency n.
 

#### Row profiles

Considering the row sums, we obtain relative frequencies for the rows, that make the profile for each row point.

The row profile for the row point '*and that*' across the social-geographical groups of teenagers is determined in the following way:
We divide the count in each cell in the row by the row sum in `N_total` of the respective row.

The calculations inside the first row are the following:
$$4 : 182, 49:182, 9:182, 44:182, 10:182, 66:182$$
```{r}

N[1,1]/N_total[1,7]
N[1,2]/N_total[1,7]
N[1,3]/N_total[1,7]
N[1,4]/N_total[1,7]
N[1,5]/N_total[1,7]
N[1,6]/N_total[1,7]

```
The percentages are used to normalize the row totals to the value 1, so that the rows can now be compared with each other regardless of the frequency of their absolute mentions. 

The first value of the row profile indicates that the extender '*and that*' is in 2% used by '*Reading_MC*' teenagers.


**Matrix of row profiles or profiles of extenders**

To obtain the row profiles for the complete table we use the `sweep()` function which is similar to `apply()`.

`sweep(x, MARGIN, STATS, FUN)` applies an operation to a data matrix over the rows or over the columns. The parameter `MARGIN =1` sweeps over the rows, while `MARGIN =2` sweeps over the columns.  STATS is the value used in the operation (e.g. 2), FUN is the type of operation (e.g. +, -).

```{r}
# sweep function divides each element of the matrix N by the sum of its respective row
# 1: The margin over which the operation is applied. 1 indicates rows, and 2 would indicate columns
# "/" indicates division operation
row.profiles = sweep(N, 1, (rowSums(N)), "/")
row.profiles
```


Note: `apply()` (returns a vector, array or a list of values by applying a function to the margins of an array (data object with possibly more than two dimensions) or of a matrix); 


The **average row** of the matrix of row profiles can be determined by the column totals of the table of margin totals `N_Total` divided by the total frequency n.

```{r}
# calculatin averages
row.averages <-  c(N_total[7,1]/n, N_total[7,2]/n, N_total[7,3]/n, N_total[7,4]/n, N_total[7,5]/n, N_total[7,6]/n)
# appending the averages to a new row
row.profiles.av <- rbind(row.profiles, row.averages)
# and naming the row
rownames(row.profiles.av) [7] <- "average row profile"
row.profiles.av
                         
```

The average row profile tells us that irrespective of the chosen extender 21% of the usages are attributed to '*Reading_MC*', 13% to '*Reading_WC*', 18% to '*Milton_Keynes_MC*', 11% to '*Milton_Keynes_WC*', 17% to '*Hull_MC*' and 16% to '*Hull_WC*' teenagers. In this way, we can compare the profiles of extenders to each other or to the average row. 
The use of the extender '*and that*' e.g. lies below the average row profile in case of '*Reading_MC*', '*Milton_Keynes_MC*' and '*Hull_MC*', and above the average row profile in case of '*Reading_WC*', '*Milton_Keynes_WC*' and '*Hull_WC*'.



#### Column profiles


Considering the column sums, we obtain relative frequencies for the columns,
that make the profile for each column point .
For example for the first column:
```{r}
N_total[1,1]/N_total[7,1]
N_total[2,1]/N_total[7,1]
N_total[3,1]/N_total[7,1]
N_total[4,1]/N_total[7,1]
N_total[5,1]/N_total[7,1]
N_total[6,1]/N_total[7,1]
```

**Matrix of column profiles**, or profiles of social-geographical groups of teenagers
```{r}
# sweep operation on columns
col.profiles = sweep(N, 2, (colSums(N)), "/")
col.profiles
```

From the column profile we grasp that in the group of '*Reading_MC*' teenagers, the extender '*and that*' was used in 2%, '*and all that*' in 2%, '*and stuff*' in 21%, '*and things*' in 18%, '*and everything*' in 12% and '*or something*' in 42% of cases. 

We again add the average column profile to the table in the last column.
```{r}
# calculate averages
col.averages <-  c(N_total[1,7]/n, N_total[2,7]/n, N_total[3,7]/n, N_total[4,7]/n, N_total[5,7]/n, N_total[6,7]/n)
# append averages to new column
col.profiles.av <- cbind(col.profiles, col.averages)
# name column
colnames(col.profiles.av) [7] <- "average column profile"

col.profiles.av
```
From the average column profile we can read that irrespective of the social-geographical groups, 23% of the instances are '*and that*', 3% '*and all that*', 22% '*and stuff*', 10% '*and things*', 17% '*and everything*', 21% '*or something*'.
We can compare the profiles of social-geographical groups to each other or to the average column. 

The use in the group of '*Hull_WC*' e.g. is above the average column for '*and that*' and '*and everything*', and below the average column for extenders '*and all that*', '*and stuff*', '*and things*', '*or something*'.

One can argue from the point of view of the row or column profiles and reach the same conclusions (symmetric view). However, the data tables are sometimes considered asymmetrically as a set of rows or set of columns, depending on the interest.

 
The information from both tables can be used to calculate the *contingency ratios* (cf. Greenacre 2007:11): those are formed from a row profile element and the corresponding average row profile or from a column profile element and the corresponding average column profile. Look at the code below for the different ways to obtain the equal ratios. In the interpretation, the ratio of '*and things*' and '*Reading_MC*' e.g. says that the combination occurs 1.7 times as much compared to the average.


```{r}
sweep(row.profiles, 2,row.averages, "/")
sweep(col.profiles, 1,col.averages, "/")
```

The calculated profiles are relevant for the geometrical representation as the following quotes by Greenacre (2007) are pointing towards:


>Profiles consisting of m elements can be plotted as points in an m-dimensional space. Because their m elements add up to 1, these profile points occupy a restricted region of this space. This region is an (m–1)-dimensional
subspace known as a simplex. This simplex is enclosed within the edges
joining all pairs of the m unit vectors on the m perpendicular axes. These
unit points are also called the vertices of the simplex or profile space. The
coordinate system within this simplex is known as the barycentric coordinate
system. (Greenacre 2007:16)


>Simplex: a triangle in two dimensions, a tetrahedron in three dimensions,
and generalizations of these geometric figures in higher dimensions; in CA J-dimensional profiles lie inside a simplex defined by J vertices in (J − 1)-
dimensional space. (Greenacre 2007:265f.)


### 1.3.2 Masses of rows and columns

The next step is to look at the concept of masses which are used as weights in correspondence analysis. 

>  Row sums or column sums divided by the total frequency of the contingency table.  
> (cf. Hautz & Bleuel 2018:221)

The sum of rows and columns of the table with the proportions is referred to as mass. 

We calculate the column masses `column.masses` by applying`colSums()`to the table of proportions.  
```{r}
P
column.masses = colSums(P)
column.masses
```

In the same way, we calculate row masses `row.masses` by using `rowSums()`:

```{r}

row.masses = rowSums(P)
row.masses
```

In the next step, we want to add the masses to the corresponding profiles. The row profile already contains the average row profile in the last row, and we want to add the row mass as a new last column.
The column profile, on the other hand, contains the average column profile as a last column and we want to add the column masses as the last row in the table.

In order to arrive at the same number of rows and columns we have to add NA values, because we don't calculate a mass value for the average row profile and average column profile.

 
```{r}

row.masses.na <-c(row.masses[0:6],NA)
row.masses.na
column.masses.na <- c(column.masses[0:6], NA)
column.masses.na
```

Remember, as the last row of the row profile we added the column sums of the correspondence matrix (P); the column sums make the average row profile and are actually identical to the column masses. 

```{r}
row.averages
as.numeric(column.masses[1:6])
```
Also, the values of the average column profile are the same as the row masses:
```{r}
col.averages
as.numeric(row.masses[1:6])
```


The average row profile is the **centroid** of the row points in the correspondence space. This does not mean the "geographic" center, but a weighted average so that the centroid is closer to points with greater weight. 

> Centroid: the weighted average point (Greenacre 2007:263)

For the geometrical representation this means:

> There is an equivalent way of thinking about the positions of the profile points in the profile space which is based on the notion of a weighted average, or centroid, of a set of points. In the calculation of an ordinary (unweighted) average, each point receives equal weight. A weighted average, on the other hand, allows different weights to be associated with each point. When points are weighted differently, then the centroid does not lie exactly at the “geographical” centre of the cloud of points, but tends to lie in a position closer to the points with higher weight.(cf. Greenacre 2007:17)


We are now in the position to add masses to the corresponding profiles:
```{r}
row.profiles.av.mass <- cbind(row.profiles.av,row.masses.na)
colnames(row.profiles.av.mass) [7] <- "row.masses" 
row.profiles.av.mass
```


The masses of the rows together form the average column profile. This is the centroid of the column points in the correspondence space. 
We also append the column masses (the row sums of the correspondence matrix P) to the column profile.
```{r}
col.profiles.av.mass <- rbind(col.profiles.av, column.masses.na)
rownames(col.profiles.av.mass)[7] <- "col.masses"
col.profiles.av.mass
```


Values of the row representation can be seen as weighted values of the column representation and the values of the column representation can be seen as weighted values of the row representation. 
For illustration, consider that a cell value is multiplied by the mass and divided by the average row (within a profile).


As an example we like to go from row profile [1,1] to column profile [1,1].
To do so we multiply the cell of the row profile by the row mass and divide by the average row. This results in the cell of the column profile.
(Alternative: cell of the row profile multiplied by row mass divided by column mass of the column profile)
$$0.02197802*0.2363636/0.2194805 = 0.02366864$$



View of both profiles
```{r}
row.profiles.av.mass
col.profiles.av.mass
```

Calculation with relevant columns of the profile
```{r}
row.profiles.av.mass[1,1]*row.profiles.av.mass[1,7]/row.profiles.av.mass[7,1]
row.profiles.av.mass[1,1]*row.profiles.av.mass[1,7]/col.profiles.av.mass[7,1]
col.profiles.av.mass[1,1]
```

To practise a little bit more, let's assume we want to go from column profile [1,1] to row profile [1,1]: Therefore, the cell of the column profile is multiplied by the column mass and divided by the average column, so we obtain the cell of the row profile.
(Alternatively, cell of the column profile multiplied by column mass divided by row mass)

$$0.02366864*0.2194805/0.2363636 =0.02197802$$


The code below illustrates the calculation. 
```{r}
col.profiles.av.mass[1,1]*col.profiles.av.mass[7,1]/col.profiles.av.mass[1,7]
col.profiles.av.mass[1,1]*col.profiles.av.mass[7,1]/row.profiles.av.mass[1,7]
row.profiles.av.mass[1,1]
```


The same holds for average profiles of rows and columns, but first let's view both profiles again.
```{r}
row.profiles.av.mass
col.profiles.av.mass
```

To calculate the 1st value of the average row profile, we multiply the components from the table of the row profile in the 1st column by the corresponding values of the average column profile and sum them up:

$$(0.02197802*0.23636364) + (0.13793103*0.03766234) + (0.20930233*0.22337662) + (0.38095238* 0.10909091) + (0.15217391*0.17922078) + (0.43636364*0.21428571) = 0.21948052	$$
```{r}
sum(row.profiles.av.mass[1:6 ,1]*col.profiles.av.mass[1:6,7])
row.profiles.av.mass[7,1]
```


To practise, let's calculate the 1st value of the average column profile. Therefore, components from the table of the 1st row column profile are multiplied by the corresponding values of the average row profile and summed up

$$(0.02366864*0.21948052) + (0.46666667*0.13636364) + (0.06293706*0.18571429) + (0.50000000*0.11428571) + (0.072463768*0.17922078) + (0.51968504*0.16493506) = 0.2363636$$


The calculation in R looks the following:
```{r}
sum(col.profiles.av.mass[1,1:6]*row.profiles.av.mass[7,1:6])
col.profiles.av.mass[1,7]
```


For the validation of calculated row and column masses, see Sec. 7.1.


# 2. Expected Proportions and Frequencies

## 2.1 Expected proportions (expected proportions) E

Referring back to Table P with the proportions, e.g. 0.5% of the examples correspond to the extender '*and that*' in the social-geographical group  '*Reading_MC*'.


```{r}
P

```


For an estimation about whether this value is large or small, we can calculate the value we expect if we assume that there is no relationship between extender and social-geographical group. 

From the sums calculated, we know that 23% of the extenders are '*and that*' (row mass) and that 21% are in the social-geographical group '*Reading MC*' (column mass). If there is no relationship between extender and social-geographical group, we expect that 21% of 23% of examples (
0.051 = 5.1%) are the extender '*and that*' in the social-geographical group '*Reading MC*'. 


```{r}
row.masses
column.masses
```

The expected proportion is calculated by multiplying the column and row masses: 
$$0.23636364*0.2194805 = 0.05187721 $$

```{r}
column.masses[1] * row.masses[1] 
```



`%o%` is used to create a table so that for each cell the corresponding row masses and column masses are multiplied. The result is table `E`
```{r}
E = row.masses %o% column.masses
E
```



## 2.2 Expected Frequencies 

> Frequencies to be expected if there is no correlation between the characteristics under consideration (null hypothesis of independence, homogeneity assumption).
(cf. Hautz & Bleuel 2018: S. 220)


Relative to the total frequency of n examples, each of which has a probability of 0.05187721 of exhibiting the combination of '*and that*' and '*Reading_MC*', n* the probability of examples are expected.

We obtain the expected frequencies by multiplying the expected proportions by the total frequency.

```{r}
Ew = E*n
Ew
```

Alternatively, we can also
calculate the expected frequency for cell ij as we practise now.


We multiply row sum i from table N with the column sum j from table N
and divide by n. 


```{r}
rowSums(N) %o% colSums(N)/n
```



# 3. Chi-squared statistics and distances 

## 3.1 Chi-square statistic

We have applied the statistical test before using the `chisq()`function, but now, we will see how to calculate it ourselves.

> chi-square statistic — the statistic used commonly for testing the indepedence model for a contingency table; calculated as the sum of squared differences between observed frequencies and frequencies expected according to the model, each squared difference being divided by the corresponding expected
frequency. (Greenacre 2007:264)

To calculate the value, we square the difference of the absolute observed and expected frequencies, divide by the expected frequencies and sum all values.


$$\chi^2 = \sum \frac {(observed - expected)^2}{expected}$$ 

cf. Greenacre (2007:27)



In the formula below, we have to insert frequencies from table N and expected frequencies from table Ew.

$$ \chi^2 = \sum^{I}_{i=1} \sum^{J}_{j=1}\frac{(n_{ij}- \hat{n}_{ij})^2}{\hat{n}_{ij}} = \frac {(4-39.945455)^2}{39.945455}+ \frac{(49 - 24.818182)^2}{24.818182}+...$$
cf. Blasius 2001:25. (n with hat corresponds to expected frequency) 


```{r}
chi2         <- sum((N-Ew)^2/Ew)
chi2
```

The larger the Chi-squared value (number of rows and columns constant), the larger the weighted squared deviations between observed and expected values. This means we are less confident that the hypothesis of independence is true and the more likely we are going to reject it.


What is the maximum value of Chi-square?

>The maximum Chi-square value is equal to the number of cases multiplied by the minimum of rows and columns minus one.

(cf. Blasius 2001:26)

```{r}
nrow(N)
ncol(N)
n*(ncol(N)-1)

```
We can check the result of our calculation with the implemented function: `chisq.test()` which will also provide the p-value.

```{r}
x2_N.test <- chisq.test(N, correct = F)
x2_N.test

```
Because of the problems emerging for Chi-squared test from the dependence on the number of cases and from the impact of the table size, coefficients like Cramer's V have been proposed. The formula is the following:

$$ V = \sqrt{\frac{\chi^2}{n * (min(I-1, J-1)}} $$
cf. Blasius 2001:26.

Cramer's V can take values between 0 (no correlation) and 1 (perfect  correlation).
```{r}
nrow(N)
ncol(N)
(min(dim(extendersdata))-1)

chi2
CramersV<-sqrt(x2_N.test$stat/(sum(extendersdata) * (min(dim(extendersdata))-1)))
CramersV
```
In correspondence analysis, a coefficient is determined analogously to Cramer's V. 



### 3.1.1  *Total Inertia*

To determine the total inertia (how much variation is present in the data), the Chi-squared value is divided by the total sum of the table or grand total.


$$\lambda_G =  \frac{\chi^2}{n}=  \frac{384.1155}{770}$$

```{r}
total.inertia = chi2/n
total.inertia

```


>Sum of squared Chi^2^ distances of a set of profiles to their centroid weighted by the masses of the profiles.
(Hautz & Bleuel 2018:222)

Alternatively, the total inertia weight is also obtained from the squared deviations of expected and observed profile elements, weighted by the corresponding average row and column profile elements.


## 3.2 Chi-squared distance

The concept of distance or (dis-)similary between variables in a correspondence analysis map is based on the chi-squared distance: the smaller Chi-squared distance between two profiles, the more similar they are.

> Chi-squared distance: weighted Euclidean distance measure between profiles,
where each squared difference between profile elements is divided by the
corresponding element of the average profile.

(cf. Greenacre 2007:263)


In the profile space, which is calculated later, the greater the Chi-squared distance of a profile from the average profile, the greater the distance of the corresponding point from the origin of the coordinate system in the profile space. Also, the greater the Chi-squared distance between two profiles, the greater the distance in space. Chi-squared distance are defined only between two row profiles or two column profiles (as well as those to the average profile). But not between row profiles and column profiles.  

Note: If you are interested, you may consider a discussion of Breitung (2023) criticizing some undesirable features of chi-squared distance when applied in correspondence analysis.

For the calculation we start by summing the squared deviations of observed and expected values of the individual cells, and divide by the respective expected values.

Chi-squared distance between row profiles (cf. Greenacre p.31)
a~ij~ : the jth element of the row profile of row i; c~j~ : the mass of the jth column.

$$\sqrt {\sum_j{\frac{(a_{ij}-a_{i'j})^2}{c_j}}}$$


If you compare it to the formula for Eucledean distance you see the difference in that the former uses weighting:

$$\sqrt {\sum_j{(a_{ij}-a_{i'j})^2}}$$




Chi-squared distance between column profiles 
b~ij~ - the i.th element of the column profile of column j; r~i~: the mass of the i.th row

$$\sqrt {\sum_i{\frac{(b_{ij}-b_{ij'})^2}{r_i}}}$$

Chi-squared distance between row profile and average row profile  

$$\sqrt {\sum_j{\frac{(a_{ij}-c_j)^2}{c_j}}}$$

Chi-squared distance between column profile and average column profile 

$$\sqrt {\sum_i{\frac{(b_{ij}-r_i)^2}{r_i}}}$$



Let's illustrate the calculation of the chi-squared distance from the first row of the row profile ('*and that*') to the average row or to the centroid of the row profile (Z)

```{r}

# Calculate the squared differences and divide by column profiles
diff1 <- (row.profiles.av.mass[1,1] - col.profiles.av.mass[7,1])^2 / col.profiles.av.mass[7,1]
diff2 <- (row.profiles.av.mass[1,2] - col.profiles.av.mass[7,2])^2 / col.profiles.av.mass[7,2]
diff3 <- (row.profiles.av.mass[1,3] - col.profiles.av.mass[7,3])^2 / col.profiles.av.mass[7,3]
diff4 <- (row.profiles.av.mass[1,4] - col.profiles.av.mass[7,4])^2 / col.profiles.av.mass[7,4]
diff5 <- (row.profiles.av.mass[1,5] - col.profiles.av.mass[7,5])^2 / col.profiles.av.mass[7,5]
diff6 <- (row.profiles.av.mass[1,6] - col.profiles.av.mass[7,6])^2 / col.profiles.av.mass[7,6]

# Sum the differences and take the square root
d2_andthat_av <- sqrt(diff1 + diff2 + diff3 + diff4 + diff5 + diff6)


d2_andthat_av
```


Note we could also use a shorter alternative (inelegant) solution with a loop: 

For j, we loop from 1 to the last number of dimensions from our table N (6) along the row profile and subtract in the first row for all columns 1-6 from the cell value the column mass of the respective column, square this and divide by the column mass of the respective column. From the values summed in chidist, we take the root. (cf. Greenacre 2007:218)

```{r}
chidist       <- 0
for(j in 1:dim(N) [2]){
  chidist <- chidist+(row.profiles.av.mass[1,j]-column.masses[j])^2/column.masses[j]
  }
sqrt(chidist)
```


Chi-squared distance between first ('*and that*') and second row ('*and all that*') of the row profile.

```{r}

# Calculate the squared differences and divide by column profiles
diff1 <- (row.profiles.av.mass[1,1] - row.profiles.av.mass[2,1])^2 / col.profiles.av.mass[7,1]
diff2 <- (row.profiles.av.mass[1,2] - row.profiles.av.mass[2,2])^2 / col.profiles.av.mass[7,2]
diff3 <- (row.profiles.av.mass[1,3] - row.profiles.av.mass[2,3])^2 / col.profiles.av.mass[7,3]
diff4 <- (row.profiles.av.mass[1,4] - row.profiles.av.mass[2,4])^2 / col.profiles.av.mass[7,4]
diff5 <- (row.profiles.av.mass[1,5] - row.profiles.av.mass[2,5])^2 / col.profiles.av.mass[7,5]
diff6 <- (row.profiles.av.mass[1,6] - row.profiles.av.mass[2,6])^2 / col.profiles.av.mass[7,6]

# Sum the differences and take the square root
d2_andthat_andallthat <- sqrt(diff1 + diff2 + diff3 + diff4 + diff5 + diff6)

d2_andthat_andallthat
```


For the calculation of all Chi-squared distances we can use the following function: 
`dist()` calculates as default a Euclidean distance matrix between the rows of a matrix.
The options set for `sweep()` are 2 (move down the columns), the root of the column masses is used as a vector, and the operation used is division (cf. Greenacre 2007:219).

```{r}
dist.matrix.row <- dist(sweep(row.profiles.av, 2, sqrt(column.masses), FUN="/")) 
dist.matrix.row

ca_validation$rowdist
```

In the above output you can compare the row chi-squared distances to the centroid (average row profile) to the retrieved solution from the ca package: `$rowdist`. Note that distances are only defined between rows (and average row). 


Now, let's analogously move on  to the Chi-squared distance between column profiles: we calculate it for the sake of illustration for the 1st column (*Reading_MC*) and average column profile or centroid of the column profile:


```{r}

# Calculate the squared differences and divide by row profiles
diff1 <- (col.profiles.av.mass[1,1] - row.profiles.av.mass[1,7])^2 / row.profiles.av.mass[1,7]
diff2 <- (col.profiles.av.mass[2,1] - row.profiles.av.mass[2,7])^2 / row.profiles.av.mass[2,7]
diff3 <- (col.profiles.av.mass[3,1] - row.profiles.av.mass[3,7])^2 / row.profiles.av.mass[3,7]
diff4 <- (col.profiles.av.mass[4,1] - row.profiles.av.mass[4,7])^2 / row.profiles.av.mass[4,7]
diff5 <- (col.profiles.av.mass[5,1] - row.profiles.av.mass[5,7])^2 / row.profiles.av.mass[5,7]
diff6 <- (col.profiles.av.mass[6,1] - row.profiles.av.mass[6,7])^2 / row.profiles.av.mass[6,7]

# Sum the differences and take the square root
d2_ReadMC_av <- sqrt(diff1 + diff2 + diff3 + diff4 + diff5 + diff6)


d2_ReadMC_av

```


In order to calculate the distance matrix for columns by using `dist()` and `sweep()`, we transpose the matrix of the column profiles. Below you can verify the correct result and compare it to the output of the $coldist variable from the ca object. 

```{r}
col.profiles.av.t <- t(col.profiles.av)
dist.matrix.col  <- dist(sweep(col.profiles.av.t, 2, sqrt(row.masses), FUN="/")) 
dist.matrix.col
ca_validation$coldist
```


# 4. Residuals, standardized Residuals

>Residuals, standardized residuals: Residuals are the differences between observed and expected frequencies. The standardized residual in row ij is obtained by dividing the residual by the square root of the expected frequency of the cell. Correspondence analysis uses standardized residuals converted to proportion values.
(cf. Hautz & Bleuel 2018: 221)

## 4.1 Residuals


The residuals are calculated by subtracting the expected proportions from the observed proportions. Normally, residuals are calculated in statistics to determine the magnitude of error in a model.

In correspondence analysis, this is different; the focus is on examining the residuals. They quantify the difference between the observed data and what we would expect if there were no relationship between row and column categories.

```{r}
R = P - E
R
```

The examples in the categories '*and that*' and '*Reading_WC*' show a high residual value of 0.031.
The observed proportion was about 6.3% which is 3% higher than the expected value of 3.2% assuming that there is no relationship between categories.

```{r}
P[1,2]
E[1,2]
R[1,2]
```

This suggests that in the example sentences of '*and that*' and '*Reading_WC*' it is more likely that the extender is used than in the average sentence.


## 4.2  Calculation of the standardized residual Z 


The standardized residual Z provides a weighting of the SVD (Singular Value Decomposition). Cells with a higher expected proportion are given a greater weight in the data. The expected values are related to the data set size. The weighting means that smaller cells in the table, for which the sampling error will be larger, are weighted lower. This makes the correspondence analysis relatively robust in dealing with outliers caused by sampling error when the table is analyzed as a contingency table. 

Formula for the calculation of an element of the table of standardized residuals: 
Deviation of observed from expected proportions divided by the root of the expected proportions.

$$a_{ij} = \frac{(p_{ij} - r_ic_j)}{\sqrt{r_ic_j}}$$  (cf. Blasius 2001:89)

```{r}
Z <- R/sqrt(E)
Z
```

If the elements of the standardized residuals are squared, multiplied by the total frequency, and then summed, the result is Chi^2^ (determined earlier):
```{r}
chi2.aus.res <- sum(((Z)^2)*n)
chi2.aus.res
chi2
```

# 5. Singular Value Decomposition 

Correspondence analysis seeks a low-dimensional subspace which should be as close as possible to all points- the points are then projected into the optimal subspace that captures as much of the variation in the data as possible. 

With respect to this objective, it is necessary to define what proximity of points to the subspace means. Intuitively, one would want to find distances of all profiles to an imaginary line for which the sum of the distances is smallest (but this involves complicated mathematics).

Instead, the problem can be simplified by defining a criterion for the sum of squared distances, which is mathematically equivalent to a least-squares method for finding an optimal subspace. 

In correspondence analysis, the weighted sum of squared distances is now taken as the criterion. (cf. Greenacre p. 47)

We assume i profile points in a multidimensional space and a low-dimensional subspace S as candidates for the optimal subspace. 
For the i.th profile point with mass m~i~ we calculate the distance between the point and S denoted by d~i~(S). The proximity of the profile to the subspace is m~i~[d~i~(S)]^2^, or the squared distance weighted by the mass. The closeness of all profiles is obtained by summation.

$$closeness\:to\:S = \sum_i {m_i[d_i(S)]^2} $$


The optimal subspace must satisfy this criterion. 

The computation of the solution of this problem can be achieved by the singular value decomposition, a procedure that is crucial for various methods of dimension reduction. In correspondence analysis, it is applied to the matrix of standardized residuals. 

**Singular value decomposition**

> a matrix decomposition similar to that of eigenvalues and eigenvectors, but applicable to rectangular matrices; the squares of the singular values are eigenvalues of particular square matrices, and the left and right singular vectors are also eigenvectors

A singular value decomposition is a generalization of the eigenvalue decomposition (for square matrices) and can be applied to any rectangular matrices.

The matrix is broken down into components, ordered from important to unimportant. The algebraic notion of *rank* of a matrix is geometrically equivalent to the notion of dimensions.

The dimensions of the optimal subspace are formed with a singular value decomposition- for this purpose a first dimension is constructed, which explains a maximum portion of the variation of the data. Then, a second dimension is constructed such that it is orthogonal to the first and again captures a maximum fraction of the remaining variation.  
In this way, the SVD successively partitions the total inertia or *total inertia* λ~G~ among the dimensions. 

The singular value decomposition also provides the coordinates of the row and column points in space in further steps of calculation.



## 5.1 Singular Value Decomposition (SVD) 

The decomposition of a matrix A is defined as the product of three matrices

$$A = U\:\Gamma\:V^T$$


Gamma is the diagonal matrix with singular values in descending order
$$\gamma_1 \geq \gamma_2 \geq...\geq \gamma_k \geq...\ge 0$$ 

In a diagonal matrix, all values except the diagonal are zeros. (For the calculation in R a representation as a vector is sufficient.) The last value is in principle 0 and in the present case, this can be ignored, as well as the last columns in u and v, which are multiplied by 0.

The columns of the matrix U are called left singular vectors and those of V are called right singular vectors. The left singular vectors correspond to the rows, and the right singular vectors correspond to the columns. The matrix V^T^ was transposed. 

Calculation is done with the `svd()` function, the resulting object is named SVD.


```{r}
SVD = svd(Z)
rownames(SVD$u) = rownames(P)
rownames(SVD$v) = colnames(P)
SVD
```



Each singular value and the corresponding vectors (e.g. the columns of u and v) correspond to one dimension. A property of U and V is that the columns of U are orthogonal to each other and that the columns of V are orthogonal to each other.


## 5.2 Eigenvalues

The squared singular values are called *eigenvalues*.

```{r}
eigenvalues = SVD$d^2
eigenvalues
```


The inertia amount which is covered by a principal axis, the so-called *principal inertia*, with reference to the first principal axis, is also called *first principal inertia*. It is also often called an eigenvalue because of the way it can be calculated, as an
eigenvalue of a square symmetric matrix.

The eigenvalue corresponds to the fraction of the total inertia accounted for by dimension k and is notated as λ~k~. The sum of the eigenvalues of all dimensions is equal to the total inertia λ~G~. 
The fraction of the total inertia accounted for by 1 or 100 percent dimension λ~k~ is called the eigenvalue fraction of the kth dimension (calculated by λ~k~/λ~G~).

[Note: *K* is the dimensionality of the matrix in space or rank, we also speak of factors and assign properties to the dimensions.
*k* is the number of factors, k is also used as an index. In terms of inertia weights, we speak of a weight~k~ as being on the kth axis, and in terms of coordinates ~k~ we speak of a coordinate on the kth factor].


The sum of the eigenvalues again gives the total inertia: (we had already determined this in 3.1.1). 

```{r}
inertia.sum = sum(eigenvalues)
inertia.sum
total.inertia
```

The eigenvalue components of the first dimensions are determined as follows: we divide each eigenvalue by the total inertia and express them as proportions.
```{r}
prop.inertias = eigenvalues/total.inertia
prop.inertias
perc.inertias<-round(prop.inertias*100, 3)
perc.inertias
```


Above, we have calculated that the first dimension of the correspondence analysis explains 73.6% of the variation in the data and the second explains 18.1%. The third dimension explains 5.1% and the fourth 3%. 

### 5.2.1 Scree-Plot

In correspondence analysis, we usually inspect a bar chart (or similar representation) with the inertia values as percentages to check for the number of dimensions to be considered for the analysis. A rule of thumb is to look for a bow in the graph and choose the dimensions before the bow. The idea is to analyse the dimensions which can account for most variation in the data. 

To create the scree-plot for the inertia values, first, the axes are counted over the length of the vector to name them.
In the second step, a bar chart of the rounded inertia values in percent is created. The bars are named using the previously defined names. 

For `ylim`, the rounded value of the largest singular value increased by 20% is chosen as the axis maximum. With `space=` the distance between the columns is determined and `las=1` labels the Y-axis horizontally. 
`text()` gets as X-coordinate a vector containing the numbers from 1 to the number of CA-axes or percent values, decreased by 0.5. As the Y-coordinate the percent values are chosen. The numbers above the bars give the percentage values. 


```{r}
nam <- paste("Axis", 1:length(perc.inertias))  
barplot(perc.inertias, names.arg = nam, col = 8,  ylim=c(0,round(max(perc.inertias)+max(perc.inertias)/5,1)),space=0, las=1)
 text((1:length(perc.inertias))-0.5, perc.inertias, perc.inertias, pos=3) 
 title(main="Principal Inertias %", font=2) 
```



## 5.3 Coordinates


### 5.3.1 Principal Coordinates

The principal coordinates indicate the coordinate positions of the profiles. Since the coordinates refer to the profiles on the principal axes, they are also called principal coordinates. 

>  Coordinates of a set of points projected onto a
principal axis, such that their weighted sum of squares along an axis equals
the principal inertia on that axis. (cf. Greenacre 2007:265)

For the localization of the i-th row on the k-th axis/factor
we calculate the product of the corresponding value of the matrix U of the left singular vectors for one row on one axis and the singular value of the k-th axis divided by the square root of the mass of the i-th row.


$$f_{ik} = u_{ik}\:\gamma_k\:/ \sqrt{r_i}$$
(cf. Blasius 2001:92)

To illustrate the procedure, we calculate the values for the position of the row '*and stuff*' on the first axis and on the second axis. Therefor we multiply the corresponding values from the matrix U of the left singular vectors and the value of the axis divided by the square root of the mass of the 3rd row.

```{r}

# position on the first axis
SVD$u[3,1]*SVD$d[1]/sqrt(row.profiles.av.mass[3,7])

# position on the second axis
SVD$u[3,2]*SVD$d[2]/sqrt(row.profiles.av.mass[3,7])


```

The complete matrix F of principal coordinates of the rows is calculated by using `sweep` again:
```{r}
pro <- sweep(SVD$u, 2, SVD$d, "*")
principal.coordinates.rows <- sweep(pro, 1, (sqrt(row.masses)), "/")
principal.coordinates.rows
```

We continue with the calculation of the principal coordinates of the columns.
For the localization of the j-th column on the k-th axis/factor
calculate the product of the corresponding value of the matrix V of the right singular vectors for a column on an axis and the singular value of the k-th axis divided by the root of the mass of the j-th column.



$$g_{jk} = v_{jk}\:\gamma_k / \sqrt{c_j}$$
Calculation of the matrix G of principal coordinates of the columns:
```{r}
pros <- sweep(SVD$v, 2, SVD$d, "*")
principal.coordinates.columns <- sweep(pros, 1, (sqrt(column.masses)), "/")
principal.coordinates.columns
```

### 5.3.2 Standard coordinates


> Coordinates of a set of points such that their weighted sum of squares along an axis equals 1. (cf. Greenacre 2007:266)


The standard coordinates stand for the coordinate positions of the vertices (in geometry corner of a polygon with at least three corners). They are standardized (mean = 0, standard deviation = 1).
As a reminder: the vertices define the boundaries of the profile space and correspond to extreme (weighted) profiles, the labels are identical with the variable values, but not their profiles. In six-dimensional space e.g. 1,0,0,0,0,0; 0,1,0,0,0,0; 0,0,1,0,0,0; 0,0,0,1,0,0; 0,0,0,0,1,0; 0,0,0,0,0,1 i.e. there would be e.g. an extender whose examples would all occur with one social-geographical group.


$$x_{ik} = u_{ik}\:/ \sqrt{r_i}$$


To calculate them, we divide each cell of the left singular vectors by the square root of the row masses. We thus obtain the standard coordinates of the rows: 

```{r}
standard.coordinates.rows = sweep(SVD$u, 1, sqrt(row.masses), "/")
standard.coordinates.rows
```

$$y_{jk} = v_{jk}\: / \sqrt{c_j} $$
The same calculation is applied to the right singular vectors, where column masses are used.


```{r}
standard.coordinates.columns = sweep(SVD$v, 1, sqrt(column.masses), "/")
standard.coordinates.columns
```

### 5.3.3 Reciprocal calculation of the coordinates

The square root of principal inertia serves as a scaling factor to get from vertex points positions' to profile point positions (cf. Greenacre 2007 p. 60).


To get from vertices to profiles, multiply by the root of principal inertia. Alternatively, we could multiply by the singular value (since the squared singular values are the eigenvalues).

$$profile\:coordinate = vertex\:coordinate\:x \sqrt {principal\:inertia}$$


In the code below, we use the column vertex point coordinate to calculate the column profile coordinates.


```{r}
principal.coordinates.columns.2 <- sweep(standard.coordinates.columns, 2, (sqrt(eigenvalues)), "*")
principal.coordinates.columns.2

principal.coordinates.columns.3 <- sweep(standard.coordinates.columns, 2, SVD$d, "*")
principal.coordinates.columns.3

principal.coordinates.columns
```


To go back from profile to vertex points multiply the profile coordinate value by the inverse of the square root of the principal inertia. (Or in simpler terms: divide the profile coordinate by the square root of the principal inertia.)



$$vertex\:coordinate = principal\:coordinate\:x\:1/\sqrt{principal\:inertia} $$
```{r}
standard.coordinates.columns.2 <- sweep(principal.coordinates.columns, 2, (1/(sqrt(eigenvalues))), "*")
standard.coordinates.columns.2

standard.coordinates.columns
```
Alternatively, the standard coordinates can be calculated from the principal coordinates by means of division by the singular values: here for the rows:


$$x_{ik} = f_{ik} / \gamma_k$$


```{r}
standard.coordinates.rows.3 <- sweep(principal.coordinates.rows, 2, SVD$d, "/")
standard.coordinates.rows.3
standard.coordinates.rows
```

For the columns:

$$y_{jk} = g_{jk}/ \gamma_k$$


```{r}
standard.coordinates.columns.3 <- sweep(principal.coordinates.columns, 2, SVD$d, "/")
standard.coordinates.columns.3
standard.coordinates.columns
```


# 6. Results: Numerical values of the analysis 
After we have calculated the coordinates, which corresponds to the information in columns k=1 and k=2 in the summary object, we will now go for the calculation of values indicating inertia weights related to axes, quality, correlation and contribution.

```{r}
summary(ca_validation)
```



## 6.1 Further inertia weights



To obtain absolute inertia weights of the rows related to the axes with marginal sums, we multiply the row mass with the squared principal coordinates
of the rows.


```{r}
abs.in.rows.ax <- sweep(principal.coordinates.rows^2, 1, row.masses, '*')
abs.in.rows.ax_total <- rbind(abs.in.rows.ax, (colSums(abs.in.rows.ax)))
rownames(abs.in.rows.ax_total) [7] <- "total"
abs.in.rows.ax_total <- cbind(abs.in.rows.ax_total, (rowSums(abs.in.rows.ax_total)))
colnames(abs.in.rows.ax_total) <- c("[,1]","[,2]","[,3]","[,4]","[,5]","[,6]","total")

abs.in.rows.ax
abs.in.rows.ax_total
```

The inertia weights of the axes are to be reconstructed from the column sums of the absolute inertia weights. 
The column sum of the first axis thus corresponds to the first eigenvalue.


```{r}
abs.in.rows.ax_total[7,1]
eigenvalues[1]
```

Display for next step in calculation:
```{r}
principal.coordinates.columns
column.masses
```



Absolute inertia weights of the columns on the axes can be obtained by
multiplication of the column mass with the squared principal coordinates of the columns.

```{r}
abs.in.cols.ax <- sweep(principal.coordinates.columns^2, 1, column.masses, '*')
abs.in.cols.ax_total <- rbind(abs.in.cols.ax, (colSums(abs.in.cols.ax)))
rownames(abs.in.cols.ax_total) [7] <- "total"
abs.in.cols.ax_total <- cbind(abs.in.cols.ax_total, (rowSums(abs.in.cols.ax_total)))
colnames(abs.in.cols.ax_total) <- c("[,1]","[,2]","[,3]","[,4]","[,5]","[,6]","total")

abs.in.cols.ax_total
```

Since the absolute inertia weights of variable expression are difficult to interpret, the relative inertia weights of variable expression are reported. 


### 6.1.1 Contribution of the rows related to the axes  (`ctr`-value of the summary) 

Relative inertia weights of the rows related to the columns can be obtained by dividing the absolute inertia weights by the column sums of absolute inertia weights.

```{r}
rel.in.rows.ax <- sweep(abs.in.rows.ax, 2, colSums(abs.in.rows.ax), '/')
rel.in.rows.ax
```
The value of relative inertia weights indicates that in the row representation the first row '*and that*' helps in explaining the geometrical alignment of the first axis by 55.6%. 
(Explanation of how much of the variation of the individual axes can be explained with the help of the variable values) The relative inertia weight related to the axes is listed in the ca summary as `ctr` for contribution.

Variation on the first axis is highly determined by '*and that*', '*and stuff*', '*and things*' , and variation on the second is highly determined by '*and stuff*', and '*or something*'.

We continue by calculating relative inertia weights of the columns with respect to the axes.


```{r}
rel.in.cols.ax <- sweep(abs.in.cols.ax, 2, colSums(abs.in.cols.ax), '/')
rel.in.cols.ax
```


From the point of view of the categories of the columns, '*Reading_MC*', '*Reading_WC*' and '*Hull_WC*' determine the first axis and '*Reading_MC*' and '*Hull_MC*' determine the second.


### 6.1.2 Inertia of the rows and columns:  `inr`- value of the summary 


In this step, we come back to the absolute inertia weights from table `abs.in.rows.ax_total`:
From the sums of the rows, it is possible to indicate what proportion of the geometric alignment of all axes each variable value has. This information can be absolute and relative:

The absolute values can already be found in the table in the row totals:


```{r}
abs.in.rows.ax_total[1:6,7]
```


To obtain the relative inertia weight, the absolute values of the row totals are divided by the total sum or the total inertia weight:

```{r}
# total sum
abs.in.rows.ax_total[7,7]
# total inertia weight
total.inertia
```

```{r}
abs.in.rows <- abs.in.rows.ax_total[1:6,7]
# calculate relative inertia weight
rel.in.rows <- abs.in.rows/abs.in.rows.ax_total[7,7]
```

```{r}
abs.in.rows

```


```{r}
rel.in.rows

```


From the table with the relative values it follows that by means of the first row '*and that*' 41.3% of the geometric alignment of the whole model can be explained. 

The relative inertia weights for a row or column are listed as `inr` in the summary of the correspondence analysis. Thus, they show how high the proportion of a category is for explaining the total variation.

You can verify the calculation by looking at the ca object.

```{r}
ca_validation$rowinertia
abs.in.rows
```

We determine analogously the relative inertia weight of the columns

```{r}
abs.in.cols <- abs.in.cols.ax_total[1:6,7]
rel.in.cols <- abs.in.cols/abs.in.cols.ax_total[7,7]
```

Validation by means of `$colinertia`
```{r}
abs.in.cols
ca_validation$colinertia

```

```{r}
rel.in.cols

```



## 6.2 Values of correlations with the axes (`cor`) and quality (`qlt` value of the summary)

If we relate the inertia weights of the rows on the axes to the sum of the inertia weights of the rows (divide by them), we can indicate how much the variable expressions are determined by the individual axes. For example, 99% of the variation of '*and that*' is explained by the first axis. 
The value for the correlation between a principal axis and a particular row or column is listed in the ca summary as `cor`.


```{r}
sqrt.fl <- abs.in.rows.ax/abs.in.rows.ax_total[1:6,7]
sqrt.fl
```

Hence, in correspondence analysis it is also possible to calculate the quality of the individual points. The farther a point is from the origin, the better the point is explained by the correspondence analysis. 

Alternatively, we can square the principal coordinates and express them as row proportions, and we get the measures of quality from each dimension for each point. These are also called squared correlations (or *squared cosines*). 



```{r}
sqrt.cor.row = prop.table(principal.coordinates.rows ^2, 1)
sqrt.cor.row

sqrt.cor.col = prop.table(principal.coordinates.columns ^2, 1)
sqrt.cor.col

sum.sqrt.cor.row <- apply(sqrt.cor.row, 1 ,sum )
sum.sqrt.cor.row
sum.sqrt.cor.col <-  apply(sqrt.cor.col, 1 ,sum )
sum.sqrt.cor.col
```


The quality of the map for a given category is usually defined as the sum of the scores it receives for two dimensions. 

In the ca summary the values are listed under `qlt`. They represent the quality of visualization or mapping quality, If the quality is low (below 50%), the categories must be interpreted with caution. 
In the example they add up to 100%.


# 7.0 Validation by `ca` package 

In this section we are now directly cross-checking our calculations to make sure we did them correctly.

## 7.1 Validation of row and column masses 

```{r}
ca_validation$rowmass
row.masses

ca_validation$colmass
column.masses
```


## 7.2 Validation of chi-squared distances

We cross-check the row and column distances against the average row and column profile.
```{r}
ca_validation$rowdist
dist.matrix.row
```
```{r}
ca_validation$coldist
dist.matrix.col
```


## 7.3 Validation of standard coordinates of row and columns 

We cross-check the variables *rowcoord* and *colcoord* of the ca object against the values we calculated. 
```{r}
ca_validation$rowcoord
standard.coordinates.rows

```


```{r}
ca_validation$colcoord
standard.coordinates.columns


```

We could also compare against the printed ca object but I prefer the above method for readability. 

## 7.4 Validation of principal coordinates 

The principal coordinates resulting from the ca-package appear when calling the summary of the ca object in the columns for individual dimensions (k=1, k=2). Note when printing the ca object itself it will show the standard coordinates instead.

```{r}
summary(ca_validation)
round(principal.coordinates.rows*1000)
round(principal.coordinates.columns*1000)
```



## 7.5 Validation of inertia-values (inertia weights) 


First, let's compare the `inr` values of relative inertia weights.


```{r}
summary(ca_validation)
print("inr rows")
round(rel.in.rows*1000)
print("inr cols")
round(rel.in.cols*1000)
```
Second, `ctr` are the relative inertia weights related to axes that we now compare.

```{r}
summary(ca_validation)
print("ctr rows")
round(rel.in.rows.ax*1000)
print("ctr cols")
round(rel.in.cols.ax*1000)

```


## 7.6 Validation of `cor` and `qlt` values 

The `cor` value appears in the column after the coordinate value in the dimensions.
```{r}
summary(ca_validation)
print("cor rows")
round(sqrt.cor.row*1000)
print("cor cols")
round(sqrt.cor.col*1000)

```

The `qlt` values are in the column after the masses of the summary. We are to compare them now.

```{r}
summary(ca_validation)

("qlt rows")
round(sum(sqrt.cor.row[1],sqrt.cor.row[7])*1000)
round(sum(sqrt.cor.row[2],sqrt.cor.row[8])*1000)
round(sum(sqrt.cor.row[3],sqrt.cor.row[9])*1000)
round(sum(sqrt.cor.row[4],sqrt.cor.row[10])*1000)
round(sum(sqrt.cor.row[5],sqrt.cor.row[11])*1000)
round(sum(sqrt.cor.row[6],sqrt.cor.row[12])*1000)

print("qlt cols")
round(sum(sqrt.cor.col[1],sqrt.cor.col[7])*1000)
round(sum(sqrt.cor.col[2],sqrt.cor.col[8])*1000)
round(sum(sqrt.cor.col[3],sqrt.cor.col[9])*1000)
round(sum(sqrt.cor.col[4],sqrt.cor.col[10])*1000)
round(sum(sqrt.cor.col[5],sqrt.cor.col[11])*1000)
round(sum(sqrt.cor.col[6],sqrt.cor.col[12])*1000)


```

In sum, we have made sure now that our calculations were done in the correct way. 

# 8. Different Visualisations


## 8.1 Method: Symmetric map: 

(Default:) The row and column points are scaled so that their inertia (weighted variance) is equal to the principal inertia (eigenvalue or singular value) along the principal axes, i.e. rows and columns are given in principal coordinates.
```{r}
plot(ca_validation, dim = c(1,2), map = "symmetric", what = c("all", "all"), 
               mass = c(FALSE, FALSE), contrib = c("none", "none"), 
               col = c("blue", "red"), 
               pch = c(16, 21, 17, 24), 
               labels = c(2, 2), 
               arrows = c(FALSE, FALSE), 
               lines = c(FALSE, FALSE), 
               lwd=1,
               xlab = "_auto_", ylab = "_auto_", 
               xlim = c(-2.0, 2.0),
               ylim = c(-1.0, 1.0),
               col.lab = c("blue", "red")) 
```


```{r}
principal.coordinates.rows
principal.coordinates.columns
```


## 8.2 Method: "rowprincipal"

Asymmetric representation: Common representation of rows and columns, the two sets of points are scaled differently (normalized), with rows in principal coordinates and columns in standard coordinates (biplots).

```{r}
plot(ca_validation, dim = c(1,2), map = "rowprincipal", what = c("all", "all"), 
               mass = c(FALSE, FALSE), contrib = c("none", "none"), 
               col = c("blue", "red"), 
               pch = c(16, 21, 17, 24), 
               labels = c(2, 2), 
               arrows = c(FALSE, FALSE), 
               lines = c(FALSE, FALSE), 
               lwd=1,
               xlab = "_auto_", ylab = "_auto_",
               col.lab = c("blue", "red")) 
```

```{r}
principal.coordinates.rows
standard.coordinates.columns
```



## 8.3 Method: "colprincipal"

asymmetric: with columns in principal coordinates and rows in standard coordinates (biplot)
```{r}
plot(ca_validation, dim = c(1,2), map = "colprincipal", what = c("all", "all"), 
               mass = c(FALSE, FALSE), contrib = c("none", "none"), 
               col = c("blue", "red"), 
               pch = c(16, 21, 17, 24), 
               labels = c(2, 2), 
               arrows = c(FALSE, FALSE), 
               lines = c(FALSE, FALSE), 
               lwd=1,
               xlab = "_auto_", ylab = "_auto_",
               col.lab = c("blue", "red")) 
```

```{r}
principal.coordinates.columns
standard.coordinates.rows
```


There are more visualization methods in the ca package, 'symbiplot', 'rowgreen' and 'colgreen', that are not discussed here.




# References
Blasius, J. (2001). *Korrespondenzanaylse*. De Gruyter. 

Breitung, J. (2023). Dr. Strangelove or: How I Learned to Stop Worrying and Love the Correspondence Analysis. URL https://wisostat.uni-koeln.de/fileadmin/sites/statistik/Celibrate_Joerg.pdf

Cheshire, J. (2007). Discourse Variation, Grammaticalisation and Stuff Like That. *Journal of Sociolinguistics 11 (2)*, 155–193.  doi:1.1111/j.1467-9841.2007.00317x. http://dx.doi.
org/10.1111/j.1467-9841.2007.00317.x.

Desagulier, G. (2017). *Corpus Linguistics and Statistics with R. Introduction to Quantitative Methods in Linguistics. Springer. doi:10.1007/978-3-319-64572-8. Additional materials: 

Greenacre, M. (2007). *Correspondence Analysis in Practice*. Chapman & Hall. 

Hautz, P. & Bleuel, F. (2018). Die Korrespondenzanalyse zur Auswertung und Visualisierung inhaltsanalytischer Daten. In: Petersen, Thomas & Schwender, Clemens (Eds), *Die Entschlüsselung der Bilder: Methoden zur Erforschung visueller Kommunikation.* (pp. 197-222). Halem.

Wasserstein, R. L. & Lazar, N. A. (2016) The ASA's Statement on p-Values: Context, Process, and Purpose, *The American Statistician, 70:2*, 129-133.
